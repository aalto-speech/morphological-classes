This project contains software for training a category n-gram model using Expectation-Maximization procedure

The form of the model is:
P(w| w-2 .. w-1) = sum_j P(w|c_j) P( c_j| w-2 .. w-1),
where j are the possible classes for the word w
P(c_j|w-2 .. w-1) = sum_s P( c_j | s) * P(s| w-2 .. w-1)
s denoting possible class sequences generated by the word sequence w-2 .. w-1
the class sequence generation probabilities are assumed to be dependent only on the word in the exact position
P(s|w-2 .. w-1) = prod P(k) (c_k | w_k)

So simply there are three parameter types:
class generation probabilities p(c|w)
class memberships p(w|c)
class n-gram p(c_i|c_{i-n-1} .. c{i-1})

MODEL TRAINING WITH OMORFI CLASS INITIALIZATION
-----------------------------------------------

(corpus.gz in iso8859-1 encoding)
gzip -dc corpus.gz |./getwords.py > words.txt
recode iso88591..utf8 words.txt
cat words.txt |omorfi-analyse-tokenised.sh > words.analysis
./omorfi_init_words.py words.analysis > words.init
recode utf8..iso88591 words.init
./train -o 100 -t 3 -m -l 60 -f 50 -c 5 words.init corpus.gz model

classseq -p 1 model.ngram.gz model.classes.gz model.words.gz corpus.gz corpus.classes.gz
ngram-count -text corpus.classes.gz -kndiscount -interpolate -order 5 -lm classes.srilm.kni.5g.arpa.gz

Other classes can be used as well, see the format of words.init file.
Currently the class probabilities p(c|w) are initialized evenly.

See train.cc for how the training proceeds. 


EXECUTABLES
-----------
train         trains the model
train2        trains the model, forces one class per word towards end of the training
catstats      -
classppl      computes class lm perplexity with an ARPA n-gram
catppl        computes category lm perplexity using the trigram model given by train (model.ngram.gz)
catppl2       computes category lm perplexity with an ARPA n-gram
catintppl     linearly interpolated perplexity (word n-gram and a category n-gram model)
catintppl2    linearly interpolated perplexity (word n-gram and two category n-gram models)
ngramppl      normal word n-gram perplexity
